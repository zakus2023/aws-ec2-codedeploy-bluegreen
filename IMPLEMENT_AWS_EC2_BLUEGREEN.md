# Step-by-Step: Implement the AWS EC2 CodeDeploy Blue/Green Project Yourself

This guide walks you through **creating the full project yourself**: repo structure, app, deploy bundle, Terraform (bootstrap + platform module + dev/prod envs), GitHub Actions, and the CrewAI crew.

- **EXPLAIN_AWS_EC2_BLUEGREEN_BEGINNER.md** — Beginner-friendly explanation of what each part does.
- **AWS_EC2_BLUEGREEN_EXPLAINED.md** — Full reference: complete code samples, repository structure (§11), sample app (§12), Docker (§13), CodeDeploy scripts (§14), Terraform split-module layout (§15), GitHub Actions YAML (§16), CrewAI orchestration (§17), Release & Deployment Pipeline (§18), and production hardening checklist (§19). Use it when you need exact file contents or the extended layout.

---

## What You Will Build

- **App**: Small Node/Express app with `/health` and `/`, run in Docker.
- **Deploy**: CodeDeploy `appspec.yml` and scripts (install, stop, start, validate) so EC2 instances pull the image from ECR and run the container.
- **Infra**:  
  - **Bootstrap**: S3 + DynamoDB (Terraform remote backend), KMS, CloudTrail bucket.  
  - **Platform module**: VPC, ALB (HTTPS + HTTP→HTTPS redirect), ACM + Route53, blue/green target groups and ASGs, CodeDeploy, ECR, SSM parameters, CloudWatch Agent config, alarms (ALB 5xx, unhealthy targets, latency, CPU, disk), security services (CloudTrail, GuardDuty, Security Hub, Inspector, Config) with account-level toggles. You can use a single `main.tf` or split into multiple files (vpc.tf, alb.tf, asg.tf, etc.) as in **AWS_EC2_BLUEGREEN_EXPLAINED.md** §11 and §15.  
  - **Envs**: `dev` and `prod`, each with its own backend config and tfvars (dev also includes root import blocks).
- **CI**: GitHub Actions — Terraform plan/apply, build-push to ECR + update SSM tag, deploy (package bundle, upload S3, trigger CodeDeploy).
- **CrewAI**: Optional crew to verify HTTPS endpoint and SSM paths (or full orchestrator from requirements). Extended design in **AWS_EC2_BLUEGREEN_EXPLAINED.md** §17.
- **Optional**: Release & Deployment Pipeline (release-crew/, release-prep.yml, CHANGELOG, deploy checklist, rollback plan, test plan) — **AWS_EC2_BLUEGREEN_EXPLAINED.md** §18. Production hardening checklist — **AWS_EC2_BLUEGREEN_EXPLAINED.md** §19.

---

## Step 1: Create the Repository and Folder Structure

Create a new repo (e.g. `aws-ec2-codedeploy-bluegreen`) and locally create the folders (and optional root files) below. **Do not** create file contents yet; just the structure.

**Full repository structure** (including split platform module files and release-crew):

```
aws-ec2-codedeploy-bluegreen/
├── README.md
├── CHANGELOG.md                  # Optional; auto-generated by release crew
├── deploy_checklist.md           # Optional; auto-generated by release crew
├── rollback_plan.md              # Optional; auto-generated by release crew
├── test_plan.md                  # Optional; auto-generated by release crew
├── app/                          # Application code
│   ├── package.json
│   ├── server.js
│   └── Dockerfile
├── deploy/                       # CodeDeploy bundle
│   ├── appspec.yml
│   └── scripts/
│       ├── install.sh
│       ├── stop.sh
│       ├── start.sh
│       └── validate.sh
├── infra/                        # Infrastructure as Code
│   ├── bootstrap/                # S3 + DynamoDB backend (run once)
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── modules/
│   │   └── platform/             # Reusable module (single main.tf OR split files below)
│   │       ├── main.tf           # Use this if you prefer one file
│   │       ├── vpc.tf            # Optional split: VPC, subnets, IGW, NAT, routes
│   │       ├── alb.tf            # Optional split: ALB, target groups, listeners
│   │       ├── asg.tf            # Optional split: launch template, ASG blue/green
│   │       ├── codedeploy.tf     # Optional split: CodeDeploy app, deployment group, S3
│   │       ├── ecr.tf            # Optional split: ECR repo, lifecycle policy
│   │       ├── ssm.tf            # Optional split: SSM parameters (ECR name, image tag)
│   │       ├── acm.tf            # Optional split: ACM certificate
│   │       ├── route53.tf        # Optional split: Route53 validation + alias
│   │       ├── cloudwatch.tf     # Optional split: log groups, CW agent config
│   │       ├── alarms.tf         # Optional split: SNS, CloudWatch alarms
│   │       ├── security.tf       # Optional split: CloudTrail + account-level security
│   │       ├── iam.tf            # Optional split: EC2 role, instance profile
│   │       ├── variables.tf
│   │       └── outputs.tf
│   └── envs/
│       ├── dev/
│       │   ├── backend.hcl
│       │   ├── imports.tf        # Root import blocks (dev)
│       │   ├── main.tf
│       │   └── dev.tfvars
│       └── prod/
│           ├── backend.hcl
│           ├── main.tf
│           └── prod.tfvars
├── .github/
│   └── workflows/
│       ├── release-prep.yml      # Optional: Release & Deployment Pipeline crew
│       ├── terraform-plan.yml    # PR: terraform plan
│       ├── terraform-apply.yml   # main: terraform apply
│       ├── build-push.yml        # main + app/: build → ECR
│       └── deploy.yml            # after build-push: CodeDeploy
├── release-crew/                 # Optional: Release preparation automation
│   ├── requirements.txt
│   ├── tools.py
│   ├── agents.py
│   ├── tasks.py
│   ├── flow.py
│   └── run_release_prep.py
└── crewai/                       # Infrastructure orchestration (verify or full orchestrator)
    ├── requirements.txt
    ├── tools.py
    ├── agents.py
    ├── flow.py
    └── run.py
```

- **Platform module**: Use either a single **main.tf** (Steps 4–5 describe contents) or the **split layout** (vpc.tf, alb.tf, asg.tf, etc.); do not mix—choose one approach.
- **Optional**: Root docs (CHANGELOG, deploy_checklist, rollback_plan, test_plan) and **release-crew/** / **release-prep.yml** are for the Release & Deployment Pipeline; see **AWS_EC2_BLUEGREEN_EXPLAINED.md** §18.

You will add file contents under each path in the following steps.

---

## Step 2: Create the App (Node + Docker)

### 2.1 `app/package.json`

Create `app/package.json` with this content:

```json
{
  "name": "bluegreen-sample",
  "version": "1.0.0",
  "main": "server.js",
  "type": "commonjs",
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.19.2"
  }
}
```

### 2.2 `app/server.js`

Create `app/server.js` with this content:

```javascript
const express = require("express");
const os = require("os");

const app = express();
const port = process.env.PORT || 8080;

// Health endpoint for ALB target group health checks
app.get("/health", (req, res) => {
  res.status(200).send("OK");
});

// Main endpoint
app.get("/", (req, res) => {
  res.json({
    message: "Hello from Blue/Green EC2 + CodeDeploy!",
    hostname: os.hostname(),
    version: process.env.APP_VERSION || "dev",
    timestamp: new Date().toISOString()
  });
});

app.listen(port, () => {
  console.log(`Server listening on port ${port}`);
});
```

- **`/health`**: ALB target group health checks use this; must return 200.
- **`/`**: Returns JSON with hostname, version (from `APP_VERSION`, set by deploy script to image tag), and timestamp.
- **Port 8080**: Matches ALB target group and Docker port mapping.

### 2.3 `app/Dockerfile`

Create `app/Dockerfile` with this content:

```dockerfile
FROM node:20-alpine

WORKDIR /usr/src/app

# Install dependencies
COPY package.json package-lock.json* ./
RUN npm ci --omit=dev || npm i --omit=dev

# Copy application code
COPY . .

# Set environment
ENV PORT=8080
EXPOSE 8080

# Start application
CMD ["npm", "start"]
```

**Build and test locally:**

```bash
cd app
docker build -t bluegreen-sample:local .
docker run -p 8080:8080 -e APP_VERSION=local bluegreen-sample:local

# In another terminal
curl http://localhost:8080/health   # Should return "OK"
curl http://localhost:8080/         # Should return JSON
```

---

## Step 3: Create the Deploy Bundle (CodeDeploy)

The **deploy bundle** is what CodeDeploy sends to each EC2 instance: an **appspec.yml** (which defines what to copy and which scripts to run) and **lifecycle scripts**. CodeDeploy copies the bundle to the instance, then runs the hooks in order. All scripts run as **root** (required for Docker and system commands). If any script exits non-zero or exceeds its **timeout**, the deployment fails and CodeDeploy can roll back.

**Hook order:** ApplicationStop → BeforeInstall → ApplicationStart → ValidateService. Only if ValidateService succeeds does CodeDeploy consider the instance healthy and (for Blue/Green) proceed to switch traffic.

---

### 3.1 `deploy/appspec.yml`

This file tells CodeDeploy where to put the bundle and which scripts to run at each lifecycle phase. Create `deploy/appspec.yml`:

```yaml
version: 0.0
os: linux

files:
  - source: /
    destination: /opt/codedeploy-bluegreen
    overwrite: true

hooks:
  ApplicationStop:
    - location: scripts/stop.sh
      timeout: 300
      runas: root

  BeforeInstall:
    - location: scripts/install.sh
      timeout: 600
      runas: root

  ApplicationStart:
    - location: scripts/start.sh
      timeout: 600
      runas: root

  ValidateService:
    - location: scripts/validate.sh
      timeout: 300
      runas: root
```

| Section | Purpose |
|--------|--------|
| **files** | Everything under `deploy/` (source `/`) is copied to `/opt/codedeploy-bluegreen` on the instance. Scripts run from that directory, so paths like `scripts/stop.sh` resolve to `/opt/codedeploy-bluegreen/scripts/stop.sh`. |
| **ApplicationStop** | Runs first: stop and remove the old app container so the new one can bind to port 8080. Timeout 300 seconds. |
| **BeforeInstall** | Ensures Docker and AWS CLI are available and the target directory exists. Timeout 600 seconds (longer in case of yum install). |
| **ApplicationStart** | Pulls the image from ECR (using SSM for repo name and tag) and starts the new container. Timeout 600 seconds (image pull can be slow). |
| **ValidateService** | Checks that `http://localhost:8080/health` returns 200. If not, deployment fails and CodeDeploy can roll back. Timeout 300 seconds. |

---

### 3.2 `deploy/scripts/install.sh`

Runs in **BeforeInstall**. Ensures the environment is ready: Docker running, AWS CLI present, and the deploy directory exists. Create `deploy/scripts/install.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Ensure Docker is running
systemctl enable docker || true
systemctl start docker || true

# Ensure AWS CLI exists (for ECR login and SSM in start.sh)
if ! command -v aws >/dev/null 2>&1; then
  yum install -y awscli
fi

# Target directory for the bundle (must match appspec files.destination)
mkdir -p /opt/codedeploy-bluegreen
```

- **set -euo pipefail**: Exit on error, undefined variables, and pipe failures.
- **systemctl enable/start docker**: Required so `docker pull` and `docker run` work in ApplicationStart.
- **aws**: Needed for ECR login and SSM in `start.sh`; user data may already install it; this is a safety net.
- **mkdir -p**: CodeDeploy will copy files here; ensuring the directory exists avoids edge cases.

---

### 3.3 `deploy/scripts/stop.sh`

Runs in **ApplicationStop** (before the new bundle is installed). Stops and removes the existing container so port 8080 is free for the new one. Create `deploy/scripts/stop.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Stop and remove old container if it exists
if docker ps -a --format '{{.Names}}' | grep -q '^sample-app$'; then
  docker rm -f sample-app || true
fi
```

- **docker ps -a --format '{{.Names}}'**: Lists container names. We only remove a container named exactly `sample-app` (our app).
- **docker rm -f**: Force-remove the container (stops it if running, then removes). `|| true` prevents the script from failing if the container was already gone.

---

### 3.4 `deploy/scripts/start.sh`

Runs in **ApplicationStart**. Reads ECR repo name and image tag from **SSM Parameter Store** (set by Terraform and updated by CI), logs into ECR, pulls the image, and runs the container. Create `deploy/scripts/start.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Environment: prod or dev (must match platform module SSM paths and CI)
# Option A: hardcode for single-env (e.g. prod only from CI)
ENV="${ENV:-prod}"

# Option B: read from a file written by user data (if you set it in launch template)
# ENV="$(cat /opt/bluegreen-env 2>/dev/null || echo prod)"

REGION="$(curl -s http://169.254.169.254/latest/meta-data/placement/region)"
ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"

# SSM paths match platform module: /bluegreen/{env}/ecr_repo_name and /bluegreen/{env}/image_tag
ECR_REPO_NAME="$(aws ssm get-parameter --name "/bluegreen/${ENV}/ecr_repo_name" --region "$REGION" --query Parameter.Value --output text)"
IMAGE_TAG="$(aws ssm get-parameter --name "/bluegreen/${ENV}/image_tag" --region "$REGION" --query Parameter.Value --output text)"

ECR_URI="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${ECR_REPO_NAME}:${IMAGE_TAG}"

aws ecr get-login-password --region "$REGION" | \
  docker login --username AWS --password-stdin "${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com"

docker pull "$ECR_URI"

docker run -d \
  --name sample-app \
  -p 8080:8080 \
  -e APP_VERSION="$IMAGE_TAG" \
  --restart always \
  "$ECR_URI"
```

| Step | Purpose |
|------|--------|
| **ENV** | Terraform and CI use env-specific SSM paths: `/bluegreen/prod/ecr_repo_name` and `/bluegreen/prod/image_tag` (and same for `dev`). Default `ENV=prod`; set env var or write `/opt/bluegreen-env` from user data if you deploy both dev and prod with the same bundle. |
| **REGION / ACCOUNT_ID** | From instance metadata and STS; used to build the ECR URI and for ECR login. |
| **SSM parameters** | Platform module creates these; CI (build-push) updates `image_tag` after pushing a new image. |
| **ECR login** | Instance profile must have `ecr:GetAuthorizationToken` and `ecr:BatchGetImage` (and read for the repo). |
| **docker run** | `--name sample-app` matches stop.sh; `-p 8080:8080` matches ALB target group; `APP_VERSION` is used by server.js. |

---

### 3.5 `deploy/scripts/validate.sh`

Runs in **ValidateService**. Verifies the app is responding on the health endpoint. If this fails, CodeDeploy marks the deployment as failed and can roll back. Create `deploy/scripts/validate.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Give the app a moment to bind to port 8080
sleep 3

STATUS="$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/health || true)"
if [[ "$STATUS" != "200" ]]; then
  echo "Health check failed, status=$STATUS"
  exit 1
fi

echo "ValidateService OK"
```

- **sleep 3**: Short delay so the container is up and listening before we curl.
- **curl -w "%{http_code}"**: Writes only the HTTP status code (e.g. 200). Non-200 or connection failure causes exit 1 and deployment failure.

---

### Make scripts executable

After creating the script files, make them executable so CodeDeploy can run them on the instance. Run this **locally**, from your **repository root** (the directory that contains `deploy/`, e.g. `aws-ec2-codedeploy-bluegreen/`):

```bash
# From repo root (e.g. aws-ec2-codedeploy-bluegreen/)
chmod +x deploy/scripts/*.sh
```

Do this **before** you commit and push. When CI (or you) packages the `deploy/` folder into a zip for CodeDeploy, the execute bits are preserved, so the scripts will be executable on the EC2 instance.

---

### Dev vs prod: SSM paths

The **platform module** creates SSM parameters per env: `/bluegreen/dev/ecr_repo_name`, `/bluegreen/dev/image_tag`, `/bluegreen/prod/ecr_repo_name`, `/bluegreen/prod/image_tag`. **start.sh** must read from the same env you are deploying:

- **Single env (e.g. prod only):** Use `ENV="${ENV:-prod}"` as in the script; no change needed if CI only deploys prod.
- **Dev and prod with same bundle:** Set `ENV` before running (e.g. CodeDeploy deployment group or user data). Option B in start.sh reads from `/opt/bluegreen-env` if you write that file in the launch template user data with the env name.

---

## Step 4: Create Infra — Bootstrap (Remote Backend + CloudTrail Bucket)

Run **once** per account/region. Bootstrap creates the **Terraform remote backend** (S3 bucket for state, DynamoDB table for locking, KMS key for encryption) and an **S3 bucket for CloudTrail** logs. Dev and prod Terraform will use these resources via `backend "s3"` and `cloudtrail_bucket`.

---

### 4.1 `infra/bootstrap/variables.tf`

Create `infra/bootstrap/variables.tf`:

```hcl
variable "project" {
  type    = string
  default = "bluegreen"
}

variable "region" {
  type    = string
  default = "us-east-1"
}
```

---

### 4.2 `infra/bootstrap/main.tf`

Create `infra/bootstrap/main.tf`:

```hcl
terraform {
  required_version = ">= 1.6.0"
  required_providers {
    aws = { source = "hashicorp/aws", version = ">= 5.0" }
  }
}

provider "aws" {
  region = var.region
}

# KMS key for state encryption
resource "aws_kms_key" "tfstate" {
  description             = "${var.project} terraform state key"
  deletion_window_in_days = 10
  enable_key_rotation     = true
}

# S3 bucket for Terraform state (bucket_prefix avoids name collisions)
resource "aws_s3_bucket" "tfstate" {
  bucket_prefix = "${var.project}-tfstate-"
  force_destroy = true
}

resource "aws_s3_bucket_versioning" "tfstate" {
  bucket = aws_s3_bucket.tfstate.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "tfstate" {
  bucket = aws_s3_bucket.tfstate.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "aws:kms"
      kms_master_key_id = aws_kms_key.tfstate.arn
    }
  }
}

resource "aws_s3_bucket_public_access_block" "tfstate" {
  bucket                  = aws_s3_bucket.tfstate.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# DynamoDB table for state locking (prevents concurrent apply)
resource "aws_dynamodb_table" "tflock" {
  name         = "${var.project}-tflock"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"
  attribute {
    name = "LockID"
    type = "S"
  }
}

# S3 bucket for CloudTrail logs (platform module will create the trail)
resource "aws_s3_bucket" "cloudtrail" {
  bucket_prefix = "${var.project}-cloudtrail-"
  force_destroy = true
}

resource "aws_s3_bucket_versioning" "cloudtrail" {
  bucket = aws_s3_bucket.cloudtrail.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_public_access_block" "cloudtrail" {
  bucket                  = aws_s3_bucket.cloudtrail.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

data "aws_caller_identity" "current" {}

# Allow CloudTrail to write logs to the bucket
data "aws_iam_policy_document" "cloudtrail_bucket_policy" {
  statement {
    sid    = "AWSCloudTrailAclCheck"
    effect = "Allow"
    actions = ["s3:GetBucketAcl"]
    resources = [aws_s3_bucket.cloudtrail.arn]
    principals {
      type        = "Service"
      identifiers = ["cloudtrail.amazonaws.com"]
    }
  }
  statement {
    sid    = "AWSCloudTrailWrite"
    effect = "Allow"
    actions = ["s3:PutObject"]
    resources = ["${aws_s3_bucket.cloudtrail.arn}/AWSLogs/${data.aws_caller_identity.current.account_id}/*"]
    principals {
      type        = "Service"
      identifiers = ["cloudtrail.amazonaws.com"]
    }
    condition {
      test     = "StringEquals"
      variable = "s3:x-amz-acl"
      values   = ["bucket-owner-full-control"]
    }
  }
}

resource "aws_s3_bucket_policy" "cloudtrail" {
  bucket     = aws_s3_bucket.cloudtrail.id
  policy     = data.aws_iam_policy_document.cloudtrail_bucket_policy.json
}
```

| Resource | Purpose |
|----------|--------|
| **KMS key** | Encrypts state at rest; rotation enabled. |
| **S3 tfstate** | Stores `terraform.tfstate` for dev/prod; versioning allows recovery. |
| **DynamoDB tflock** | Lock table; only one apply at a time per state. |
| **S3 cloudtrail** | Holds CloudTrail logs; platform module references this bucket by name. |
| **CloudTrail bucket policy** | Lets `cloudtrail.amazonaws.com` GetBucketAcl and PutObject (with ACL condition) under `AWSLogs/<account_id>/*`. |

---

### 4.3 `infra/bootstrap/outputs.tf`

Create `infra/bootstrap/outputs.tf`:

```hcl
output "tfstate_bucket" {
  value = aws_s3_bucket.tfstate.bucket
}

output "tflock_table" {
  value = aws_dynamodb_table.tflock.name
}

output "tfstate_kms" {
  value = aws_kms_key.tfstate.arn
}

output "cloudtrail_bucket" {
  value = aws_s3_bucket.cloudtrail.bucket
}
```

**Where these outputs go:** After bootstrap, Terraform prints four output values. You must plug them into your **dev** and **prod** config so those environments know which state bucket and lock table to use, and which CloudTrail bucket the platform module should use.

| Bootstrap output    | Where it goes | Why |
|---------------------|---------------|-----|
| **tfstate_bucket**  | `infra/envs/dev/backend.hcl` and `infra/envs/prod/backend.hcl` → `bucket` | Dev and prod state files are stored in this S3 bucket (e.g. under keys `dev/terraform.tfstate` and `prod/terraform.tfstate`). |
| **tflock_table**    | Same two `backend.hcl` files → `dynamodb_table` | Terraform uses this DynamoDB table to lock state so only one apply runs at a time. |
| **cloudtrail_bucket** | `infra/envs/dev/dev.tfvars` and `infra/envs/prod/prod.tfvars` → variable `cloudtrail_bucket` | The platform module creates a CloudTrail trail that writes logs to this bucket; each env’s tfvars pass the bucket name into the module. |

**tfstate_kms** is optional to copy: it’s the KMS key ARN that encrypts state. You only need it if you reference it elsewhere (e.g. IAM); backend config does not require it.

**Concretely:**  
1. In **dev/backend.hcl** and **prod/backend.hcl**, set `bucket` = the **tfstate_bucket** value and `dynamodb_table` = the **tflock_table** value.  
2. In **dev.tfvars** and **prod.tfvars**, set `cloudtrail_bucket` = the **cloudtrail_bucket** value.  
Until you do this, `terraform init -backend-config=backend.hcl -reconfigure` in dev or prod will fail (bucket/table not found) or use wrong resources.

---

### 4.4 Run bootstrap

From your **repository root**:

```bash
cd infra/bootstrap
terraform init
terraform apply -auto-approve
```

Then run `terraform output` and fill in the backend config and tfvars as below.

---

### 4.5 Fill backend.hcl (dev and prod) after bootstrap

The **backend.hcl** file configures the S3 backend for Terraform. Terraform loads it with `terraform init -backend-config=backend.hcl -reconfigure`. Each env uses the **same** S3 bucket and DynamoDB table (from bootstrap) but a **different state key** so dev and prod state are stored separately.

**Create `infra/envs/dev/backend.hcl`** (create the file now with placeholders; replace placeholders after bootstrap):

```hcl
bucket         = "REPLACE_WITH_BOOTSTRAP_TFSTATE_BUCKET"
key            = "dev/terraform.tfstate"
region         = "us-east-1"
dynamodb_table = "REPLACE_WITH_BOOTSTRAP_TFLOCK_TABLE"
encrypt        = true
```

**Create `infra/envs/prod/backend.hcl`** (same as dev except `key`):

```hcl
bucket         = "REPLACE_WITH_BOOTSTRAP_TFSTATE_BUCKET"
key            = "prod/terraform.tfstate"
region         = "us-east-1"
dynamodb_table = "REPLACE_WITH_BOOTSTRAP_TFLOCK_TABLE"
encrypt        = true
```

| Key | Purpose |
|-----|---------|
| **bucket** | S3 bucket where state is stored. Replace with the **tfstate_bucket** output from `terraform output` (run in `infra/bootstrap/`). |
| **key** | Object key inside the bucket. Dev uses `dev/terraform.tfstate`, prod uses `prod/terraform.tfstate`. |
| **region** | AWS region; must match where the bucket and table exist (e.g. `us-east-1`). |
| **dynamodb_table** | DynamoDB table used for state locking. Replace with the **tflock_table** output from bootstrap. |
| **encrypt** | State file is encrypted at rest (S3 SSE; bootstrap enables this). |

**After bootstrap:** Replace `REPLACE_WITH_BOOTSTRAP_TFSTATE_BUCKET` and `REPLACE_WITH_BOOTSTRAP_TFLOCK_TABLE` in **both** files with the actual values from `cd infra/bootstrap && terraform output`. Also set **cloudtrail_bucket** in **dev.tfvars** and **prod.tfvars** to the **cloudtrail_bucket** output.

---

## Step 5: Create Infra — Platform Module (Split Files)

**What is the platform module?** It’s the Terraform that creates one **environment** (dev or prod): VPC, load balancer, HTTPS, EC2 instances in Auto Scaling Groups, CodeDeploy, ECR, monitoring, and security services. Dev and prod each call this module with different variables (different domain, VPC CIDR, size, etc.).

**Why split into many files?** A single huge `main.tf` is hard to read and edit. Splitting by **topic** (VPC in one file, ALB in another, alarms in another) makes it easier to find and change things. Terraform loads all `.tf` files in the folder together, so the split is only for humans.

**Order to create files:** Create the files below in `infra/modules/platform/` in any order; Terraform doesn’t care. A logical order is: **variables.tf** → **outputs.tf** → **vpc.tf** → **iam.tf** → **acm.tf** → **route53.tf** → **alb.tf** → **ecr.tf** → **ssm.tf** → **cloudwatch.tf** → **asg.tf** → **codedeploy.tf** → **alarms.tf** → **security.tf**.

---

### 5.1 `infra/modules/platform/variables.tf`

**What it does:** Declares all inputs to the module. The **env** (dev/prod) root passes these in from `main.tf` and `*.tfvars`.

Create `infra/modules/platform/variables.tf`:

```hcl
variable "project" { type = string }
variable "region"  { type = string }
variable "env"     { type = string } # "dev" or "prod"

variable "domain_name"    { type = string }
variable "hosted_zone_id" { type = string }
variable "alarm_email"   { type = string }

variable "vpc_cidr"        { type = string }
variable "public_subnets"  { type = list(string) }
variable "private_subnets" { type = list(string) }

variable "instance_type"    { type = string }
variable "min_size"         { type = number }
variable "max_size"         { type = number }
variable "desired_capacity" { type = number }

variable "ami_id" {
  type    = string
  default = ""
}

variable "cloudtrail_bucket" { type = string }

variable "enable_guardduty" {
  type    = bool
  default = true
}

variable "enable_securityhub" {
  type    = bool
  default = true
}

variable "enable_inspector2" {
  type    = bool
  default = true
}

variable "enable_config" {
  type    = bool
  default = true
}
```

---

### 5.2 `infra/modules/platform/outputs.tf`

**What it does:** Values this module exposes to the caller (e.g. `codedeploy_app`, `artifacts_bucket`). CI and deploy scripts use these.

Create `infra/modules/platform/outputs.tf`:

```hcl
output "alb_dns_name"     { value = aws_lb.app.dns_name }
output "app_domain"       { value = var.domain_name }
output "https_url"        { value = "https://${var.domain_name}" }
output "ecr_repo"         { value = aws_ecr_repository.app.name }
output "codedeploy_app"   { value = aws_codedeploy_app.app.name }
output "codedeploy_group" { value = aws_codedeploy_deployment_group.dg.deployment_group_name }
output "artifacts_bucket" { value = aws_s3_bucket.artifacts.bucket }
```

---

### 5.3 `infra/modules/platform/vpc.tf`

**What it does:** Network layer: VPC, public/private subnets, Internet Gateway, NAT Gateway, route tables, and **security groups** (ALB: 80/443 from internet; EC2: 8080 from ALB only).

Create `infra/modules/platform/vpc.tf`:

```hcl
data "aws_availability_zones" "az" {}

resource "aws_vpc" "this" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  tags = { Name = "${var.project}-${var.env}-vpc" }
}

resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.this.id
  tags   = { Name = "${var.project}-${var.env}-igw" }
}

resource "aws_subnet" "public" {
  count                   = length(var.public_subnets)
  vpc_id                  = aws_vpc.this.id
  cidr_block              = var.public_subnets[count.index]
  availability_zone       = data.aws_availability_zones.az.names[count.index]
  map_public_ip_on_launch = true
  tags = { Name = "${var.project}-${var.env}-public-${count.index}" }
}

resource "aws_subnet" "private" {
  count             = length(var.private_subnets)
  vpc_id            = aws_vpc.this.id
  cidr_block        = var.private_subnets[count.index]
  availability_zone = data.aws_availability_zones.az.names[count.index]
  tags = { Name = "${var.project}-${var.env}-private-${count.index}" }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.this.id
  tags   = { Name = "${var.project}-${var.env}-public-rt" }
}

resource "aws_route" "public_default" {
  route_table_id         = aws_route_table.public.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.igw.id
}

resource "aws_route_table_association" "public" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

resource "aws_eip" "nat" { domain = "vpc" }

resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat.id
  subnet_id     = aws_subnet.public[0].id
  depends_on    = [aws_internet_gateway.igw]
  tags = { Name = "${var.project}-${var.env}-nat" }
}

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.this.id
  tags   = { Name = "${var.project}-${var.env}-private-rt" }
}

resource "aws_route" "private_default" {
  route_table_id         = aws_route_table.private.id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = aws_nat_gateway.nat.id
}

resource "aws_route_table_association" "private" {
  count          = length(aws_subnet.private)
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

# Security group for ALB: allow HTTP/HTTPS from internet
resource "aws_security_group" "alb_sg" {
  name   = "${var.project}-${var.env}-alb-sg"
  vpc_id = aws_vpc.this.id
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Security group for EC2: allow 8080 only from ALB
resource "aws_security_group" "ec2_sg" {
  name   = "${var.project}-${var.env}-ec2-sg"
  vpc_id = aws_vpc.this.id
  ingress {
    from_port       = 8080
    to_port         = 8080
    protocol        = "tcp"
    security_groups = [aws_security_group.alb_sg.id]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

---

### 5.4 `infra/modules/platform/iam.tf`

**What it does:** IAM role and instance profile for EC2 so instances can pull from ECR, read SSM, and send logs to CloudWatch.

Create `infra/modules/platform/iam.tf`:

```hcl
data "aws_iam_policy_document" "ec2_assume" {
  statement {
    actions = ["sts:AssumeRole"]
    principals { type = "Service" identifiers = ["ec2.amazonaws.com"] }
  }
}

resource "aws_iam_role" "ec2_role" {
  name               = "${var.project}-${var.env}-ec2-role"
  assume_role_policy = data.aws_iam_policy_document.ec2_assume.json
}

resource "aws_iam_role_policy_attachment" "ssm" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

resource "aws_iam_role_policy_attachment" "ecr" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role_policy_attachment" "cw_agent" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
}

resource "aws_iam_instance_profile" "ec2_profile" {
  name = "${var.project}-${var.env}-ec2-profile"
  role = aws_iam_role.ec2_role.name
}
```

---

### 5.5 `infra/modules/platform/acm.tf`

**What it does:** Requests an HTTPS certificate for your domain. AWS validates ownership via DNS (Route53 records in the next file).

Create `infra/modules/platform/acm.tf`:

```hcl
resource "aws_acm_certificate" "cert" {
  domain_name       = var.domain_name
  validation_method = "DNS"
  lifecycle { create_before_destroy = true }
  tags = { Name = "${var.project}-${var.env}-cert" }
}
```

---

### 5.6 `infra/modules/platform/route53.tf`

**What it does:** Creates the DNS records ACM needs to validate the certificate, then creates an **alias** record so your domain points at the ALB.

Create `infra/modules/platform/route53.tf`:

```hcl
resource "aws_route53_record" "cert_validation" {
  for_each = {
    for dvo in aws_acm_certificate.cert.domain_validation_options : dvo.domain_name => {
      name   = dvo.resource_record_name
      record = dvo.resource_record_value
      type   = dvo.resource_record_type
    }
  }
  zone_id = var.hosted_zone_id
  name    = each.value.name
  type    = each.value.type
  ttl     = 60
  records = [each.value.record]
}

resource "aws_acm_certificate_validation" "cert" {
  certificate_arn         = aws_acm_certificate.cert.arn
  validation_record_fqdns = [for r in aws_route53_record.cert_validation : r.fqdn]
}

resource "aws_route53_record" "app_alias" {
  zone_id = var.hosted_zone_id
  name    = var.domain_name
  type    = "A"
  alias {
    name                   = aws_lb.app.dns_name
    zone_id                = aws_lb.app.zone_id
    evaluate_target_health = true
  }
}
```

---

### 5.7 `infra/modules/platform/alb.tf`

**What it does:** Application Load Balancer in public subnets, two target groups (blue/green), HTTP listener that redirects to HTTPS, and HTTPS listener that forwards to the blue target group (CodeDeploy switches this to green on deploy).

Create `infra/modules/platform/alb.tf`:

```hcl
resource "aws_lb" "app" {
  name               = "${var.project}-${var.env}-alb"
  internal           = false
  load_balancer_type = "application"
  subnets            = aws_subnet.public[*].id
  security_groups    = [aws_security_group.alb_sg.id]
}

resource "aws_lb_target_group" "blue" {
  name     = "${var.project}-${var.env}-tg-blue"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = aws_vpc.this.id
  health_check {
    path                = "/health"
    matcher             = "200"
    interval            = 15
    timeout             = 5
    healthy_threshold   = 2
    unhealthy_threshold = 3
  }
}

resource "aws_lb_target_group" "green" {
  name     = "${var.project}-${var.env}-tg-green"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = aws_vpc.this.id
  health_check {
    path                = "/health"
    matcher             = "200"
    interval            = 15
    timeout             = 5
    healthy_threshold   = 2
    unhealthy_threshold = 3
  }
}

resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.app.arn
  port              = 80
  protocol          = "HTTP"
  default_action {
    type = "redirect"
    redirect {
      port        = "443"
      protocol    = "HTTPS"
      status_code = "HTTP_301"
    }
  }
}

resource "aws_lb_listener" "https" {
  load_balancer_arn = aws_lb.app.arn
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS13-1-2-2021-06"
  certificate_arn   = aws_acm_certificate_validation.cert.certificate_arn
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.blue.arn
  }
}
```

---

### 5.8 `infra/modules/platform/ecr.tf`

**What it does:** Docker image repository and lifecycle policy (keep last 20 images).

Create `infra/modules/platform/ecr.tf`:

```hcl
resource "aws_ecr_repository" "app" {
  name                 = "${var.project}-${var.env}-app"
  image_tag_mutability = "IMMUTABLE"
}

resource "aws_ecr_lifecycle_policy" "app" {
  repository = aws_ecr_repository.app.name
  policy = jsonencode({
    rules = [{
      rulePriority = 1
      description  = "Keep last 20 images"
      selection = {
        tagStatus   = "any"
        countType   = "imageCountMoreThan"
        countNumber = 20
      }
      action = { type = "expire" }
    }]
  })
}
```

---

### 5.9 `infra/modules/platform/ssm.tf`

**What it does:** SSM parameters that store the ECR repo name and the current image tag. The deploy script and CI read/update these.

Create `infra/modules/platform/ssm.tf`:

```hcl
resource "aws_ssm_parameter" "ecr_repo_name" {
  name  = "/bluegreen/${var.env}/ecr_repo_name"
  type  = "String"
  value = aws_ecr_repository.app.name
}

resource "aws_ssm_parameter" "image_tag" {
  name  = "/bluegreen/${var.env}/image_tag"
  type  = "String"
  value = "initial"
}
```

---

### 5.10 `infra/modules/platform/cloudwatch.tf`

**What it does:** Log groups for Docker and system logs, and an SSM parameter with the CloudWatch Agent config so instances can ship logs to CloudWatch.

Create `infra/modules/platform/cloudwatch.tf`:

```hcl
locals {
  cw_agent_config = jsonencode({
    logs = {
      logs_collected = {
        files = {
          collect_list = [
            {
              file_path       = "/var/lib/docker/containers/*/*.log"
              log_group_name  = "/${var.project}/${var.env}/docker"
              log_stream_name = "{instance_id}"
              timezone        = "UTC"
            },
            {
              file_path       = "/var/log/messages"
              log_group_name  = "/${var.project}/${var.env}/system"
              log_stream_name = "{instance_id}"
              timezone        = "UTC"
            }
          ]
        }
      }
    }
    metrics = {
      metrics_collected = {
        cpu = {
          measurement                 = ["cpu_usage_idle", "cpu_usage_user", "cpu_usage_system"]
          metrics_collection_interval = 60
          totalcpu                   = true
        }
        disk = {
          measurement                 = ["used_percent"]
          metrics_collection_interval = 60
          resources                   = ["*"]
        }
        mem = {
          measurement                 = ["mem_used_percent"]
          metrics_collection_interval = 60
        }
      }
      append_dimensions = {
        InstanceId           = "$${aws:InstanceId}"
        AutoScalingGroupName = "$${aws:AutoScalingGroupName}"
      }
    }
  })
}

resource "aws_cloudwatch_log_group" "docker" {
  name              = "/${var.project}/${var.env}/docker"
  retention_in_days = 14
}

resource "aws_cloudwatch_log_group" "system" {
  name              = "/${var.project}/${var.env}/system"
  retention_in_days = 14
}

resource "aws_ssm_parameter" "cw_agent_config" {
  name  = "/${var.project}/${var.env}/cloudwatch/agent-config"
  type  = "String"
  value = local.cw_agent_config
}
```

---

### 5.11 `infra/modules/platform/asg.tf`

**What it does:** Launch template (Amazon Linux 2, Docker + CodeDeploy agent + CloudWatch agent in user data), and **two Auto Scaling Groups** (blue = active, green = standby until CodeDeploy switches traffic).

Create `infra/modules/platform/asg.tf`:

```hcl
data "aws_ami" "al2" {
  most_recent = true
  owners      = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}

locals {
  ami = var.ami_id != "" ? var.ami_id : data.aws_ami.al2.id
  user_data = <<-EOF
    #!/bin/bash
    set -e
    yum update -y
    yum install -y docker ruby wget amazon-cloudwatch-agent
    systemctl enable docker
    systemctl start docker
    REGION=$(curl -s http://169.254.169.254/latest/meta-data/placement/region)
    cd /home/ec2-user
    wget https://aws-codedeploy-${var.region}.s3.${var.region}.amazonaws.com/latest/install
    chmod +x ./install
    ./install auto
    systemctl start codedeploy-agent
    /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
      -a fetch-config -m ec2 \
      -c ssm:/${var.project}/${var.env}/cloudwatch/agent-config -s
  EOF
}

resource "aws_launch_template" "lt" {
  name_prefix   = "${var.project}-${var.env}-lt-"
  image_id      = local.ami
  instance_type = var.instance_type
  iam_instance_profile {
    name = aws_iam_instance_profile.ec2_profile.name
  }
  vpc_security_group_ids = [aws_security_group.ec2_sg.id]
  user_data              = base64encode(local.user_data)
  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "${var.project}-${var.env}-app"
      Env  = var.env
    }
  }
}

resource "aws_autoscaling_group" "blue" {
  name                = "${var.project}-${var.env}-asg-blue"
  min_size            = var.min_size
  max_size            = var.max_size
  desired_capacity    = var.desired_capacity
  vpc_zone_identifier = aws_subnet.private[*].id
  target_group_arns   = [aws_lb_target_group.blue.arn]
  launch_template {
    id      = aws_launch_template.lt.id
    version = "$Latest"
  }
  tag {
    key                 = "Name"
    value               = "${var.project}-${var.env}-blue"
    propagate_at_launch = true
  }
  tag {
    key                 = "Env"
    value               = var.env
    propagate_at_launch = true
  }
}

resource "aws_autoscaling_group" "green" {
  name                = "${var.project}-${var.env}-asg-green"
  min_size            = 0
  max_size            = var.max_size
  desired_capacity    = 0
  vpc_zone_identifier = aws_subnet.private[*].id
  target_group_arns   = [aws_lb_target_group.green.arn]
  launch_template {
    id      = aws_launch_template.lt.id
    version = "$Latest"
  }
  tag {
    key                 = "Name"
    value               = "${var.project}-${var.env}-green"
    propagate_at_launch = true
  }
  tag {
    key                 = "Env"
    value               = var.env
    propagate_at_launch = true
  }
}
```

---

### 5.12 `infra/modules/platform/codedeploy.tf`

**What it does:** S3 bucket for deployment bundles, CodeDeploy application and **Blue/Green deployment group** (both ASGs, HTTPS listener, auto rollback on failure or alarm).

Create `infra/modules/platform/codedeploy.tf`:

```hcl
resource "aws_s3_bucket" "artifacts" {
  bucket_prefix = "${var.project}-${var.env}-codedeploy-"
  force_destroy = true
}

resource "aws_s3_bucket_public_access_block" "artifacts" {
  bucket                  = aws_s3_bucket.artifacts.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

data "aws_iam_policy_document" "codedeploy_assume" {
  statement {
    actions = ["sts:AssumeRole"]
    principals { type = "Service" identifiers = ["codedeploy.amazonaws.com"] }
  }
}

resource "aws_iam_role" "codedeploy_role" {
  name               = "${var.project}-${var.env}-codedeploy-role"
  assume_role_policy = data.aws_iam_policy_document.codedeploy_assume.json
}

resource "aws_iam_role_policy_attachment" "codedeploy" {
  role       = aws_iam_role.codedeploy_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole"
}

resource "aws_codedeploy_app" "app" {
  name             = "${var.project}-${var.env}-codedeploy-app"
  compute_platform = "Server"
}

resource "aws_codedeploy_deployment_group" "dg" {
  app_name              = aws_codedeploy_app.app.name
  deployment_group_name  = "${var.project}-${var.env}-dg"
  service_role_arn      = aws_iam_role.codedeploy_role.arn
  autoscaling_groups    = [aws_autoscaling_group.blue.name, aws_autoscaling_group.green.name]
  deployment_style {
    deployment_type   = "BLUE_GREEN"
    deployment_option = "WITH_TRAFFIC_CONTROL"
  }
  blue_green_deployment_config {
    deployment_ready_option {
      # STOP_DEPLOYMENT allows wait_time; CONTINUE_DEPLOYMENT cannot have a timeout
      action_on_timeout    = "STOP_DEPLOYMENT"
      wait_time_in_minutes = 10
    }
    terminate_blue_instances_on_deployment_success {
      action                           = "TERMINATE"
      termination_wait_time_in_minutes = 5
    }
    green_fleet_provisioning_option { action = "DISCOVER_EXISTING" }
  }
  load_balancer_info {
    target_group_pair_info {
      prod_traffic_route { listener_arns = [aws_lb_listener.https.arn] }
      target_group { name = aws_lb_target_group.blue.name }
      target_group { name = aws_lb_target_group.green.name }
    }
  }
  auto_rollback_configuration {
    enabled = true
    events  = ["DEPLOYMENT_FAILURE", "DEPLOYMENT_STOP_ON_ALARM", "DEPLOYMENT_STOP_ON_REQUEST"]
  }
  alarm_configuration {
    enabled = true
    alarms  = [aws_cloudwatch_metric_alarm.alb_5xx.alarm_name, aws_cloudwatch_metric_alarm.unhealthy_hosts.alarm_name]
  }
}
```

---

### 5.13 `infra/modules/platform/alarms.tf`

**What it does:** SNS topic for email alerts and CloudWatch alarms (ALB 5xx, unhealthy targets, latency, CPU, disk).

Create `infra/modules/platform/alarms.tf`:

```hcl
resource "aws_sns_topic" "alerts" {
  name = "${var.project}-${var.env}-alerts"
}

resource "aws_sns_topic_subscription" "email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alarm_email
}

resource "aws_cloudwatch_metric_alarm" "alb_5xx" {
  alarm_name          = "${var.project}-${var.env}-alb-5xx"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "HTTPCode_ELB_5XX_Count"
  namespace           = "AWS/ApplicationELB"
  period              = 60
  statistic           = "Sum"
  threshold           = 5
  dimensions = { LoadBalancer = aws_lb.app.arn_suffix }
  alarm_actions = [aws_sns_topic.alerts.arn]
}

resource "aws_cloudwatch_metric_alarm" "unhealthy_hosts" {
  alarm_name          = "${var.project}-${var.env}-unhealthy-targets"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "UnHealthyHostCount"
  namespace           = "AWS/ApplicationELB"
  period              = 60
  statistic           = "Average"
  threshold           = 0
  dimensions = {
    LoadBalancer = aws_lb.app.arn_suffix
    TargetGroup  = aws_lb_target_group.blue.arn_suffix
  }
  alarm_actions = [aws_sns_topic.alerts.arn]
}

resource "aws_cloudwatch_metric_alarm" "latency" {
  alarm_name          = "${var.project}-${var.env}-alb-latency"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "TargetResponseTime"
  namespace           = "AWS/ApplicationELB"
  period              = 60
  statistic           = "Average"
  threshold           = 1.5
  dimensions = { LoadBalancer = aws_lb.app.arn_suffix }
  alarm_actions = [aws_sns_topic.alerts.arn]
}

resource "aws_cloudwatch_metric_alarm" "cpu" {
  alarm_name          = "${var.project}-${var.env}-ec2-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 60
  statistic           = "Average"
  threshold           = 80
  alarm_actions       = [aws_sns_topic.alerts.arn]
}

resource "aws_cloudwatch_metric_alarm" "disk" {
  alarm_name          = "${var.project}-${var.env}-disk-used"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "disk_used_percent"
  namespace           = "CWAgent"
  period              = 60
  statistic           = "Average"
  threshold           = 85
  alarm_actions       = [aws_sns_topic.alerts.arn]
}
```

---

### 5.14 `infra/modules/platform/security.tf`

**What it does:** CloudTrail (uses the bootstrap bucket), plus account-level security services. In this implementation, account-level services are enabled once (dev) and disabled in prod via `enable_*` flags.

Create `infra/modules/platform/security.tf`:

```hcl
data "aws_caller_identity" "current" {}

resource "aws_cloudtrail" "trail" {
  name                          = "${var.project}-${var.env}-trail"
  s3_bucket_name                = var.cloudtrail_bucket
  include_global_service_events = true
  is_multi_region_trail         = true
  enable_log_file_validation    = true
}

resource "aws_guardduty_detector" "gd" {
  count  = var.enable_guardduty ? 1 : 0
  enable = true
}

resource "aws_securityhub_account" "sh" {
  count                    = var.enable_securityhub ? 1 : 0
  enable_default_standards = false
}

resource "aws_inspector2_enabler" "insp" {
  count          = var.enable_inspector2 ? 1 : 0
  account_ids    = [data.aws_caller_identity.current.account_id]
  resource_types = ["EC2", "ECR"]
}

resource "aws_iam_role" "config_role" {
  count = var.enable_config ? 1 : 0
  name  = "${var.project}-${var.env}-config-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = { Service = "config.amazonaws.com" }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "config" {
  count      = var.enable_config ? 1 : 0
  role       = aws_iam_role.config_role[0].name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWS_ConfigRole"
}

resource "aws_s3_bucket" "config" {
  count         = var.enable_config ? 1 : 0
  bucket_prefix = "${var.project}-${var.env}-config-"
  force_destroy = true
}

resource "aws_s3_bucket_public_access_block" "config" {
  count                   = var.enable_config ? 1 : 0
  bucket                  = aws_s3_bucket.config[0].id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Required: Config needs bucket policy to deliver snapshots; incl. ListBucket per AWS docs
resource "aws_s3_bucket_policy" "config" {
  count  = var.enable_config ? 1 : 0
  bucket = aws_s3_bucket.config[0].id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid       = "AWSConfigBucketPermissionsCheck"
        Effect    = "Allow"
        Principal = { Service = "config.amazonaws.com" }
        Action    = "s3:GetBucketAcl"
        Resource  = "arn:aws:s3:::${aws_s3_bucket.config[0].bucket}"
        Condition = { StringEquals = { "AWS:SourceAccount" = data.aws_caller_identity.current.account_id } }
      },
      {
        Sid       = "AWSConfigBucketExistenceCheck"
        Effect    = "Allow"
        Principal = { Service = "config.amazonaws.com" }
        Action    = "s3:ListBucket"
        Resource  = "arn:aws:s3:::${aws_s3_bucket.config[0].bucket}"
        Condition = { StringEquals = { "AWS:SourceAccount" = data.aws_caller_identity.current.account_id } }
      },
      {
        Sid       = "AWSConfigBucketDelivery"
        Effect    = "Allow"
        Principal = { Service = "config.amazonaws.com" }
        Action    = "s3:PutObject"
        Resource  = "arn:aws:s3:::${aws_s3_bucket.config[0].bucket}/config/AWSLogs/${data.aws_caller_identity.current.account_id}/Config/*"
        Condition = {
          StringEquals = {
            "s3:x-amz-acl"      = "bucket-owner-full-control"
            "AWS:SourceAccount" = data.aws_caller_identity.current.account_id
          }
        }
      }
    ]
  })
}

resource "aws_config_configuration_recorder" "rec" {
  count    = var.enable_config ? 1 : 0
  name     = "${var.project}-${var.env}-recorder"
  role_arn = aws_iam_role.config_role[0].arn
  recording_group {
    all_supported                 = true
    include_global_resource_types = true
  }
}

resource "aws_config_delivery_channel" "chan" {
  count          = var.enable_config ? 1 : 0
  name           = "${var.project}-${var.env}-channel"
  s3_bucket_name = aws_s3_bucket.config[0].bucket
  s3_key_prefix  = "config"
  depends_on     = [aws_config_configuration_recorder.rec[0], aws_s3_bucket_policy.config[0]]
}

resource "aws_config_configuration_recorder_status" "rec_status" {
  count      = var.enable_config ? 1 : 0
  name       = aws_config_configuration_recorder.rec[0].name
  is_enabled = true
  depends_on = [aws_config_delivery_channel.chan[0]]
}
```

---

### Summary: platform module files

| File | Purpose |
|------|--------|
| **variables.tf** | Inputs (project, env, domain, VPC, sizes, cloudtrail_bucket, enable_* flags). |
| **outputs.tf** | Values used by CI and deploy (ecr_repo, codedeploy_app, artifacts_bucket, https_url). |
| **vpc.tf** | VPC, subnets, IGW, NAT, routes, ALB and EC2 security groups. |
| **iam.tf** | EC2 role (SSM, ECR, CloudWatch Agent) and instance profile. |
| **acm.tf** | HTTPS certificate request. |
| **route53.tf** | Certificate validation records and alias to ALB. |
| **alb.tf** | ALB, blue/green target groups, HTTP redirect, HTTPS listener. |
| **ecr.tf** | ECR repo and lifecycle policy. |
| **ssm.tf** | ECR repo name and image tag parameters. |
| **cloudwatch.tf** | Log groups and CloudWatch Agent config in SSM. |
| **asg.tf** | Launch template (user data: Docker, CodeDeploy, CW agent), blue and green ASGs. |
| **codedeploy.tf** | Artifacts bucket, CodeDeploy app, Blue/Green deployment group. |
| **alarms.tf** | SNS topic, email subscription, CloudWatch alarms. |
| **security.tf** | CloudTrail, GuardDuty, Security Hub, Inspector, AWS Config. |

After creating all files, run `terraform init` and `terraform validate` from an env folder (e.g. `infra/envs/dev`) that uses this module to confirm there are no syntax or reference errors.

---

## Step 6: Create Infra — Dev and Prod Envs

**What this step does:** You already have the **platform module** (Step 5) that defines VPC, ALB, EC2, CodeDeploy, etc. Here you create **two environments** that *use* that module: **dev** and **prod**. Each environment has:

- **backend.hcl** — Tells Terraform where to store state (same S3 bucket, different key: `dev/terraform.tfstate` vs `prod/terraform.tfstate`). You created these in **section 4.5**; after bootstrap, replace the placeholders with real bucket and table names.
- **main.tf** — Declares the S3 backend, the AWS provider, and a single **module "platform"** call with all variables. Dev and prod use the same structure; only the variable values (and `env`) differ.
- **variables.tf** — Declares the variables that the env root module accepts (so you can pass them from tfvars).
- **dev.tfvars** / **prod.tfvars** — The actual values per environment (domain, VPC CIDR, instance counts, alarm email, etc.). Dev typically has a smaller footprint; prod has a separate VPC and higher capacity.

You run `terraform init -backend-config=backend.hcl -reconfigure` and `terraform apply -var-file=...` **once per environment**. Dev and prod are fully separate: different state files, different AWS resources.

---

### 6.1–6.2 Backend config (already in 4.5)

Create **backend.hcl** in both `infra/envs/dev/` and `infra/envs/prod/` using the content in **section 4.5**. Use `key = "dev/terraform.tfstate"` in dev and `key = "prod/terraform.tfstate"` in prod. After you run bootstrap (Step 4.4), replace `REPLACE_WITH_BOOTSTRAP_TFSTATE_BUCKET` and `REPLACE_WITH_BOOTSTRAP_TFLOCK_TABLE` with the outputs from `terraform output` in `infra/bootstrap/`.

---

### 6.3 `infra/envs/dev/main.tf`

**What it does:** Defines the **root module** for the dev environment: backend (state in S3), AWS provider, and one call to the platform module with `env = "dev"` and all other variables coming from `var.*` (filled from **dev.tfvars**).

Create `infra/envs/dev/main.tf`:

```hcl
terraform {
  required_version = ">= 1.6.0"
  required_providers {
    aws = { source = "hashicorp/aws", version = ">= 5.0" }
  }
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

module "platform" {
  source = "../../modules/platform"

  project        = var.project
  region         = var.region
  env            = "dev"
  domain_name    = var.domain_name
  hosted_zone_id = var.hosted_zone_id
  alarm_email    = var.alarm_email

  vpc_cidr        = var.vpc_cidr
  public_subnets  = var.public_subnets
  private_subnets = var.private_subnets

  instance_type    = var.instance_type
  min_size         = var.min_size
  max_size         = var.max_size
  desired_capacity = var.desired_capacity

  ami_id = var.ami_id

  cloudtrail_bucket = var.cloudtrail_bucket

  enable_guardduty  = var.enable_guardduty
  enable_securityhub = var.enable_securityhub
  enable_inspector2 = var.enable_inspector2
  enable_config     = var.enable_config
}
```

- **backend "s3" {}** — Backend type is S3; the actual bucket, key, and DynamoDB table come from **backend.hcl** when you run `terraform init -backend-config=backend.hcl -reconfigure`.
- **module "platform"** — Calls the platform module; **source** is the path from this file to the module folder. Every argument (project, region, env, …) must match a variable in the module; we pass through the env root’s variables so one **dev.tfvars** file controls all values.

---

### 6.4 `infra/envs/prod/main.tf`

**What it does:** Same as dev, but with **env = "prod"**. All other variables come from **prod.tfvars**, so prod gets its own domain, VPC CIDR, and sizes.

Create `infra/envs/prod/main.tf`:

```hcl
terraform {
  required_version = ">= 1.6.0"
  required_providers {
    aws = { source = "hashicorp/aws", version = ">= 5.0" }
  }
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

module "platform" {
  source = "../../modules/platform"

  project        = var.project
  region         = var.region
  env            = "prod"
  domain_name    = var.domain_name
  hosted_zone_id = var.hosted_zone_id
  alarm_email    = var.alarm_email

  vpc_cidr        = var.vpc_cidr
  public_subnets  = var.public_subnets
  private_subnets = var.private_subnets

  instance_type    = var.instance_type
  min_size         = var.min_size
  max_size         = var.max_size
  desired_capacity = var.desired_capacity

  ami_id = var.ami_id

  cloudtrail_bucket = var.cloudtrail_bucket

  enable_guardduty  = var.enable_guardduty
  enable_securityhub = var.enable_securityhub
  enable_inspector2 = var.enable_inspector2
  enable_config     = var.enable_config
}
```

Only **env** is different (`"prod"`). All other inputs are still `var.*` and will be set by **prod.tfvars**. (If you prefer to hardcode a few values in prod, you can replace the corresponding `var.*` with literals.)

---

### 6.5 `infra/envs/dev/variables.tf` and `infra/envs/dev/dev.tfvars`

**What they do:** **variables.tf** declares the variables the dev root module accepts so Terraform knows their types; you can then set them in **dev.tfvars** (or via `-var`). Using **-var-file=dev.tfvars** loads all values at once so you don’t type them on every command.

---

**If you already have a Route53 hosted zone (e.g. my-iifb.click):**

1. **Get your Hosted zone ID**  
   - In the AWS Console go to **Route 53** → **Hosted zones**.  
   - Click your zone (e.g. **my-iifb.click**).  
   - On the **Hosted zone details** page you’ll see **Hosted zone ID** (e.g. `Z0123456789ABCDEFGHIJ`). Copy that value.

2. **Choose subdomains for dev and prod**  
   You don’t “get” a subdomain from anywhere — you **choose** it. Because you own the hosted zone **my-iifb.click**, any name under it is yours; Terraform will create the DNS record when you apply.  
   - Pick a **label** for dev (e.g. `dev-app`, `dev`, `staging`) and one for prod (e.g. `app`, `www`, `api`).  
   - The full domain is then **your-label.my-iifb.click** (e.g. dev-app.my-iifb.click, app.my-iifb.click).  
   - Examples: **dev-app.my-iifb.click** (dev), **app.my-iifb.click** (prod). You can use **www.my-iifb.click** for prod if you prefer.  
   - You do **not** create these records in Route53 first. The platform module will create:  
     - The **ACM validation** CNAME record(s) for the certificate.  
     - An **A (alias)** record for your chosen name pointing to the ALB.

3. **Fill 6.5**  
   - In **dev.tfvars**: set **domain_name** = your dev subdomain (e.g. `"dev-app.my-iifb.click"`) and **hosted_zone_id** = the ID you copied (e.g. `"Z04241223G31RGIMMIL2C"`).  
   - In **prod.tfvars**: set **domain_name** = your prod subdomain (e.g. `"app.my-iifb.click"`) and the **same hosted_zone_id** (one zone can hold multiple records/subdomains).

4. **Existing records**  
   - The zone already has NS, SOA, and possibly other records (e.g. an ACM validation CNAME). Leave them as they are. Terraform will add only the new records it needs for this project (certificate validation + alias to ALB).

---

Create `infra/envs/dev/variables.tf`:

```hcl
variable "project"        { type = string }
variable "region"         { type = string }
variable "domain_name"    { type = string }
variable "hosted_zone_id" { type = string }
variable "alarm_email"    { type = string }
variable "cloudtrail_bucket" { type = string }

variable "vpc_cidr"        { type = string }
variable "public_subnets"  { type = list(string) }
variable "private_subnets" { type = list(string) }

variable "instance_type"    { type = string }
variable "min_size"         { type = number }
variable "max_size"         { type = number }
variable "desired_capacity" { type = number }

variable "ami_id" { type = string default = "" }

variable "enable_guardduty"  { type = bool default = true }
variable "enable_securityhub" { type = bool default = true }
variable "enable_inspector2" { type = bool default = true }
variable "enable_config"     { type = bool default = true }

variable "enable_guardduty"  { type = bool default = true }
variable "enable_securityhub" { type = bool default = true }
variable "enable_inspector2" { type = bool default = true }
variable "enable_config"     { type = bool default = true }
```

Create `infra/envs/dev/dev.tfvars`. Replace every placeholder with your own values; **cloudtrail_bucket** (and, for backend.hcl, **tfstate_bucket** and **tflock_table**) must come from **bootstrap outputs** after you run Step 4.4.

```hcl
# Project and region (match bootstrap; region must be same as where bootstrap ran)
project = "bluegreen"
region  = "us-east-1"

# Domain and DNS: use a subdomain of your Route53 zone (e.g. my-iifb.click → dev-app.my-iifb.click)
# Get hosted_zone_id from Route53 → Hosted zones → click your zone (e.g. my-iifb.click) → copy "Hosted zone ID"
domain_name    = "dev-app.my-iifb.click"
hosted_zone_id = "Z04241223G31RGIMMIL2C"
alarm_email    = "idbsch2012@gmail.com"

# From bootstrap output: run "terraform output" in infra/bootstrap/ and copy cloudtrail_bucket
cloudtrail_bucket = "bluegreen-cloudtrail-20260207024017739500000001"

# VPC and subnets for dev (use a different CIDR than prod, e.g. 10.20.x for dev, 10.30.x for prod)
vpc_cidr       = "10.20.0.0/16"
public_subnets = ["10.20.1.0/24", "10.20.2.0/24"]
private_subnets = ["10.20.11.0/24", "10.20.12.0/24"]

# Instance size and ASG: dev = 1 instance, prod can be 2+ for HA
instance_type    = "t3.micro"
min_size         = 1
max_size         = 2
desired_capacity = 1

# Leave empty to use latest Amazon Linux 2; or set a specific AMI id
ami_id = ""

# Account-level services (managed once; keep enabled in dev)
enable_guardduty  = true
enable_securityhub = true
enable_inspector2 = true
enable_config     = true
```

**Variable-by-variable:**

| Variable | Where to get it | Example / note |
|----------|------------------|----------------|
| **project** | Your choice; used in resource names | `"bluegreen"` |
| **region** | Same region as bootstrap | `"us-east-1"` |
| **domain_name** | Dev subdomain in your zone (Terraform will create the A record) | `"dev-app.my-iifb.click"` |
| **hosted_zone_id** | Route53 → Hosted zones → click your zone (e.g. my-iifb.click) → copy **Hosted zone ID** | `"Z04241223G31RGIMMIL2C"` |
| **alarm_email** | Email for SNS alarm notifications | `"idbsch2012@gmail.com"` |
| **cloudtrail_bucket** | **Bootstrap output** after `terraform apply` in `infra/bootstrap/` | `"bluegreen-cloudtrail-20260207024017739500000001"` |
| **vpc_cidr** | Pick a CIDR for dev; don’t overlap prod | `"10.20.0.0/16"` |
| **public_subnets** | Two /24s inside vpc_cidr (for ALB) | `["10.20.1.0/24", "10.20.2.0/24"]` |
| **private_subnets** | Two /24s inside vpc_cidr (for EC2) | `["10.20.11.0/24", "10.20.12.0/24"]` |
| **instance_type** | EC2 type | `"t3.micro"` for dev |
| **min_size** / **max_size** / **desired_capacity** | ASG sizes; dev often 1/2/1 | `1`, `2`, `1` |
| **ami_id** | Optional; leave `""` for latest Amazon Linux 2 | `""` |

**After you run bootstrap (Step 4.4):** In `infra/bootstrap/` run:

```bash
terraform output
```

You’ll see something like (exact names depend on your run):

```
cloudtrail_bucket = "bluegreen-cloudtrail-20260207024017739500000001"
tflock_table      = "bluegreen-tflock"
tfstate_bucket    = "bluegreen-tfstate-20260207024017739500000002"
tfstate_kms       = "arn:aws:kms:us-east-1:058264482067:key/865a1525-4fac-4be4-a144-bd96fa0fe641"
```

**Concrete example** — If your bootstrap output is exactly the above, use these values:

- **infra/envs/dev/backend.hcl** (and **prod/backend.hcl** with `key = "prod/terraform.tfstate"` for prod):

```hcl
bucket         = "bluegreen-tfstate-20260207024017739500000002"
key            = "dev/terraform.tfstate"
region         = "us-east-1"
dynamodb_table = "bluegreen-tflock"
encrypt        = true
```

- **infra/envs/dev/dev.tfvars** (and **prod.tfvars** for prod) — set **cloudtrail_bucket**:

```hcl
cloudtrail_bucket = "bluegreen-cloudtrail-20260207024017739500000001"
```

(Keep all other dev.tfvars / prod.tfvars entries as in the full examples above; only this line must match bootstrap output.)

- Copy **cloudtrail_bucket** into **dev.tfvars** and **prod.tfvars**.
- Copy **tfstate_bucket** and **tflock_table** into **dev/backend.hcl** and **prod/backend.hcl** as `bucket` and `dynamodb_table` (see section 4.5).

Until you replace `REPLACE_WITH_BOOTSTRAP_CLOUDTRAIL_BUCKET` and fill backend.hcl, `terraform init` or `terraform apply` in envs will fail or use wrong resources.

---

### 6.6 `infra/envs/prod/variables.tf` and `infra/envs/prod/prod.tfvars`

**What they do:** Prod uses the same variable names as dev so the same **main.tf** and module call work. **variables.tf** can be a copy of dev’s; **prod.tfvars** holds prod-specific values (different domain, VPC CIDR, and usually larger sizes).

Create `infra/envs/prod/variables.tf` (same as dev):

```hcl
variable "project"        { type = string }
variable "region"         { type = string }
variable "domain_name"    { type = string }
variable "hosted_zone_id" { type = string }
variable "alarm_email"    { type = string }
variable "cloudtrail_bucket" { type = string }

variable "vpc_cidr"        { type = string }
variable "public_subnets"  { type = list(string) }
variable "private_subnets" { type = list(string) }

variable "instance_type"    { type = string }
variable "min_size"         { type = number }
variable "max_size"         { type = number }
variable "desired_capacity" { type = number }

variable "ami_id" { type = string default = "" }
```

Create `infra/envs/prod/prod.tfvars` (replace placeholders; use bootstrap output for **cloudtrail_bucket**):

```hcl
project = "bluegreen"
region  = "us-east-1"

domain_name    = "app.my-iifb.click"
hosted_zone_id = "Z04241223G31RGIMMIL2C"
alarm_email    = "idbsch2012@gmail.com"
cloudtrail_bucket = "bluegreen-cloudtrail-20260207024017739500000001"

vpc_cidr       = "10.30.0.0/16"
public_subnets = ["10.30.1.0/24", "10.30.2.0/24"]
private_subnets = ["10.30.11.0/24", "10.30.12.0/24"]

instance_type    = "t3.micro"
min_size         = 2
max_size         = 4
desired_capacity = 2

ami_id = ""

# Account-level services already enabled elsewhere (disable in prod)
enable_guardduty  = false
enable_securityhub = false
enable_inspector2 = false
enable_config     = false
```

- Prod uses a **different VPC CIDR** (e.g. 10.30.x) so it doesn’t overlap with dev (10.20.x).
- **desired_capacity** and **max_size** are higher for availability. Set **domain_name** and **hosted_zone_id** to your production domain and zone.

---

### 6.7 Check variable names

The platform module expects **hosted_zone_id** (this guide). Some docs use **route53_zone_id**; the module’s variable name in Step 5 is **hosted_zone_id**, so keep that in **main.tf** and **variables.tf**. All other names match the module (project, region, env, domain_name, alarm_email, vpc_cidr, subnets, instance_type, min_size, max_size, desired_capacity, ami_id, cloudtrail_bucket, enable_* flags).

---

### 6.8 Run Terraform for dev, then prod

**What this does:** Initializes the S3 backend and applies the platform module once per environment. Dev and prod do not share state; each apply only touches its own resources.

From your **repository root**:

```bash
# Dev
cd infra/envs/dev
terraform init -backend-config=backend.hcl -reconfigure
terraform apply -auto-approve -var-file=dev.tfvars

# Prod (from repo root again, or cd ../prod from dev)
cd infra/envs/prod
terraform init -backend-config=backend.hcl -reconfigure
terraform apply -auto-approve -var-file=prod.tfvars
```

- **terraform init -backend-config=backend.hcl -reconfigure** — Reads backend.hcl and configures the S3 backend (bucket, key, DynamoDB table). Run once per env directory (or whenever you change backend config).
- **terraform apply -auto-approve -var-file=dev.tfvars** — Applies the plan using variable values from **dev.tfvars**; **-auto-approve** skips the confirmation prompt. Use **prod.tfvars** in the prod directory.

After both applies, you have two stacks: dev (e.g. dev-app.example.com) and prod (e.g. app.example.com), each with its own VPC, ALB, ASGs, and CodeDeploy.

---

## Step 7: Create GitHub Actions Workflows (Detailed, Step‑by‑Step)

### 7.0 Prerequisites (one‑time)

1. **Create an AWS OIDC role for GitHub Actions**
   - In AWS, go to **IAM → Identity providers → Add provider**.
   - Select **OpenID Connect** and use:
     - **Provider URL**: `https://token.actions.githubusercontent.com`
     - **Audience**: `sts.amazonaws.com`
   - Create an IAM role that **trusts** this provider.
     - Condition should allow your **GitHub org/repo** (e.g. `repo:OWNER/REPO:*`).
   - Attach permissions the workflows need:
     - **Terraform**: S3 (state + artifacts), DynamoDB (lock), IAM, EC2, ELB/ALB, ACM, Route53, CloudWatch, SNS, SSM, ECR, CodeDeploy.
   - **Terraform alternative (recommended)** — create the OIDC provider + role as code:
     1. Create a new folder: `infra/oidc/`
     2. Create `infra/oidc/main.tf`:
        ```hcl
        terraform {
          required_version = ">= 1.6.0"
          required_providers {
            aws = { source = "hashicorp/aws", version = ">= 5.0" }
          }
        }

        provider "aws" {
          region = var.region
        }

        data "aws_iam_policy_document" "github_oidc_assume_role" {
          statement {
            actions = ["sts:AssumeRoleWithWebIdentity"]
            principals {
              type        = "Federated"
              identifiers = [aws_iam_openid_connect_provider.github.arn]
            }
            condition {
              test     = "StringEquals"
              variable = "token.actions.githubusercontent.com:aud"
              values   = ["sts.amazonaws.com"]
            }
            condition {
              test     = "StringLike"
              variable = "token.actions.githubusercontent.com:sub"
              values   = ["repo:${var.github_org}/${var.github_repo}:*"]
            }
          }
        }

        resource "aws_iam_openid_connect_provider" "github" {
          url             = "https://token.actions.githubusercontent.com"
          client_id_list  = ["sts.amazonaws.com"]
          thumbprint_list = ["6938fd4d98bab03faadb97b34396831e3780aea1"]
        }

        resource "aws_iam_role" "github_actions" {
          name               = var.role_name
          assume_role_policy = data.aws_iam_policy_document.github_oidc_assume_role.json
        }

        resource "aws_iam_role_policy" "github_actions" {
          name = "${var.role_name}-policy"
          role = aws_iam_role.github_actions.id
          policy = jsonencode({
            Version = "2012-10-17"
            Statement = [
              {
                Effect = "Allow"
                Action = [
                  "s3:*",
                  "dynamodb:*",
                  "iam:*",
                  "ec2:*",
                  "elasticloadbalancing:*",
                  "autoscaling:*",
                  "acm:*",
                  "route53:*",
                  "logs:*",
                  "cloudwatch:*",
                  "sns:*",
                  "ssm:*",
                  "ecr:*",
                  "codedeploy:*"
                ]
                Resource = "*"
              }
            ]
          })
        }
        ```
     3. Create `infra/oidc/variables.tf`:
        ```hcl
        variable "region" {
          type    = string
          default = "us-east-1"
        }

        variable "github_org" {
          type = string
        }

        variable "github_repo" {
          type = string
        }

        variable "role_name" {
          type    = string
          default = "github-actions-bluegreen"
        }
        ```
     4. Create `infra/oidc/outputs.tf`:
        ```hcl
        output "role_arn" {
          value = aws_iam_role.github_actions.arn
        }
        ```
     5. Run:
        - `cd infra/oidc`
        - `terraform init`
        - `terraform apply -auto-approve -var github_org=OWNER -var github_repo=REPO`
     6. Copy the output `role_arn` into the GitHub secret `AWS_ROLE_TO_ASSUME`.
     7. Keep `AWS_REGION=us-east-1` in GitHub secrets.

2. **Add GitHub repo secrets**
   - In GitHub, open your repo → **Settings → Secrets and variables → Actions**.
   - Add:
     - `AWS_ROLE_TO_ASSUME` = the **ARN** of the IAM role you just created.
     - `AWS_REGION` = `us-east-1`.
   - **Where to see them later**:
     - Repo level: **Settings → Secrets and variables → Actions → Repository secrets**
     - Org level (if used): **Settings → Secrets and variables → Actions → Organization secrets**

3. **Apply bootstrap once**
   - From your repo root:
     - `cd infra/bootstrap`
     - `terraform init`
     - `terraform apply -auto-approve`
   - Copy the outputs into:
     - `infra/envs/dev/backend.hcl`
     - `infra/envs/prod/backend.hcl`
     - `cloudtrail_bucket` in `dev.tfvars` and `prod.tfvars`

4. **Apply dev and prod once**
   - Dev:
     - `cd infra/envs/dev`
     - `terraform init -backend-config=backend.hcl -reconfigure`
     - `terraform apply -auto-approve -var-file=dev.tfvars`
   - Prod:
     - `cd infra/envs/prod`
     - `terraform init -backend-config=backend.hcl -reconfigure`
     - `terraform apply -auto-approve -var-file=prod.tfvars`

5. **Confirm prerequisites are in place**
   - **S3 state bucket** exists and contains `dev/terraform.tfstate` and `prod/terraform.tfstate`.
   - **ECR repo** exists for prod (`bluegreen-prod-app`).
   - **SSM parameters** exist:
     - `/bluegreen/prod/ecr_repo_name`
     - `/bluegreen/prod/image_tag`

### 7.1 Workflow: Terraform Plan (PR)

**Goal**: show infrastructure changes before merge.

**File**: `.github/workflows/terraform-plan.yml`

**Trigger**: `pull_request` on `infra/**`

**Steps**:
1. Checkout code.
2. Configure AWS via OIDC using `AWS_ROLE_TO_ASSUME`.
3. Install Terraform.
4. In `infra/envs/dev`:
   - `terraform init -backend-config=backend.hcl -reconfigure`
   - `terraform fmt -check`
   - `terraform validate`
   - `terraform plan -var-file=dev.tfvars`

**Why dev?** Safer to preview changes in a lower environment. (You can switch to prod if you prefer.)

### 7.2 Workflow: Terraform Apply (main)

**Goal**: apply approved infra changes.

**File**: `.github/workflows/terraform-apply.yml`

**Trigger**: `push` to `main` on `infra/**`

**Steps**:
1. Checkout code.
2. Configure AWS via OIDC.
3. Install Terraform.
4. In `infra/envs/prod`:
   - `terraform init -backend-config=backend.hcl -reconfigure`
   - `terraform apply -auto-approve -var-file=prod.tfvars`

**Tip**: Use GitHub environments with `environment: production` to add approvals.

### 7.3 Workflow: Build + Push Docker Image

**Goal**: build the app image and publish to ECR.

**File**: `.github/workflows/build-push.yml`

**Trigger**: `push` to `main` on `app/**`

**Steps**:
1. Checkout code.
2. Configure AWS via OIDC.
3. Read ECR repo name from SSM:
   - `/bluegreen/prod/ecr_repo_name`
4. ECR login.
5. Build image from `app/`, tag with `${GITHUB_SHA::12}`.
6. Push image to ECR.
7. Update SSM param `/bluegreen/prod/image_tag` with the same tag (**overwrite**).

### 7.4 Workflow: Deploy via CodeDeploy

**Goal**: ship the latest image to prod using CodeDeploy.

**File**: `.github/workflows/deploy.yml`

**Trigger**: `workflow_run` of **build-push** (when conclusion is success)

**Steps**:
1. Checkout code.
2. Configure AWS via OIDC.
3. In `infra/envs/prod`:
   - `terraform init -backend-config=backend.hcl -reconfigure`
   - `terraform output` to fetch:
     - `codedeploy_app`
     - `codedeploy_group`
     - `artifacts_bucket`
     - `https_url`
4. Zip `deploy/` into `deployment-<sha>.zip`.
5. Upload to S3 `s3://<artifacts_bucket>/revisions/deployment-<sha>.zip`.
6. `aws deploy create-deployment` with that S3 location.
7. Print the `https_url` in the job summary.

### 7.5 Permissions checklist (OIDC role)

Ensure the role can:
- Read/write **state**: S3 + DynamoDB
- Manage **infra**: EC2, ELB, ASG, ACM, Route53, IAM, CloudWatch, SNS, SSM, ECR, CodeDeploy
- Read **SSM** values used in CI (`/bluegreen/prod/ecr_repo_name`, `/bluegreen/prod/image_tag`)

### 7.6 Full YAML examples

Full workflow YAML samples (plan/apply/build‑push/deploy) are in **AWS_EC2_BLUEGREEN_EXPLAINED.md** §16.


---

## Step 8: Create the CrewAI Crew (Optional)

CrewAI can support two roles for this project (see **EXPLAIN_AWS_EC2_BLUEGREEN_BEGINNER.md**, section 9):

- **Verifier (recommended first)** — The crew only **checks** that stages exist: HTTPS health, SSM parameters, CloudWatch log groups, alarms, security services. It does not create resources; you create them with Terraform and CI.
- **Orchestrator (advanced)** — The crew **drives** creation: agents with Code Interpreter and file tools generate Terraform/config and run `terraform init/apply` and `aws` CLI so that HTTPS, CloudWatch, alarms, Inspector/GuardDuty/Security Hub/CloudTrail/Config, and dev/prod + backend are created. Use with care (e.g. human approval for `terraform apply`).

Below is the **verifier** setup; you can extend it later to orchestrator (add tools that run Terraform and AWS commands).

**8.1** In `crewai/`, create **requirements.txt** with `crewai`, `crewai-tools` (and optional `requests`, `boto3` if you call HTTPS or AWS in a tool).

**8.2** In `crewai/`, create **tools.py**: define a tool (e.g. **HTTP health check**) that, given a URL (e.g. `https://app.example.com/health`), uses `requests.get(url, verify=True, timeout=10)` and returns status code and a short summary. Optionally a tool to **read SSM parameters** (boto3 or AWS CLI via subprocess) for `/bluegreen/prod/ecr_repo_name` and `/bluegreen/prod/image_tag` so the crew can verify they are set. For **orchestrator**, add tools that run Terraform (e.g. in a given directory with a given var-file) or AWS CLI/API (create alarm, enable GuardDuty, etc.); protect destructive actions with approval or sandbox.

**8.3** In `crewai/`, create **agents.py**: define one or two agents (e.g. “Deployment verifier”) with a goal like “Verify the HTTPS endpoint returns 200 and SSM parameters are set for the prod image.”

**8.4** In `crewai/`, create **flow.py**: define the crew with the agents and tasks (e.g. task: “Check HTTPS health and SSM parameters”; expected output: short report). For orchestrator, add tasks in order: bootstrap → dev → prod (or verify-only tasks for each stage).

**8.5** In `crewai/`, create **run.py**: load inputs (e.g. prod URL from env or argument), kick off the crew, print or save the result. Use **env** for AWS region and optional credentials if the crew runs outside CI.

You can later replace this with a full CrewAI project (e.g. `crewai create crew ...`) and move tools/agents into the standard config/crew structure; the guide assumes a minimal standalone `crewai/` folder for simplicity.

**Extended CrewAI design** (TerraformTool, DockerECRTool, SSMTool, CodeDeployTool, VerifyTool; manager, infra_engineer, build_engineer, deploy_engineer; full flow: generate → bootstrap → dev → prod → build → deploy → verify) is in **AWS_EC2_BLUEGREEN_EXPLAINED.md** §17.

---

## Step 9: Run Order (Exact Sequence)

1. **Bootstrap (once)**  
   `cd infra/bootstrap` → `terraform init` → `terraform apply -auto-approve`  
   Copy outputs into `infra/envs/dev/backend.hcl`, `infra/envs/prod/backend.hcl`, and into `cloudtrail_bucket` in both **dev.tfvars** and **prod.tfvars**.

2. **Deploy DEV**  
  `cd infra/envs/dev` → `terraform init -backend-config=backend.hcl -reconfigure` → `terraform apply -auto-approve -var-file=dev.tfvars`.

3. **Deploy PROD**  
  `cd infra/envs/prod` → `terraform init -backend-config=backend.hcl -reconfigure` → `terraform apply -auto-approve -var-file=prod.tfvars`.

4. **Push app to main**  
   CI runs **build-push** (build image, push ECR, update SSM tag) then **deploy** (package deploy bundle, upload S3, CodeDeploy deployment). Confirm SNS email subscription for alarms if you use email.

5. **Validate**  
   Open `https://<your-prod-domain>/health` (and `/`). Check CloudWatch Logs for `/${project}/${env}/docker` and alarms in the console.

---

## Step 10: Crew as Full Orchestrator (Optional — User Provides Requirements, Crew Writes Everything)

If you want the **crew to orchestrate and confirm**: the user supplies **requirements**, and the crew **writes** all Terraform files, Dockerfile, app, deploy bundle, and GitHub Actions, then **confirms** (validates and optionally verifies after apply). Use this as an alternative to creating the repo by hand (Steps 1–9).

### 10.1 Input: user requirements

Define a single **requirements** input (e.g. JSON or a structured prompt) that the crew consumes. Include:

| Field | Example | Used for |
|-------|--------|----------|
| `project` | `"bluegreen"` | Resource names, prefixes. |
| `region` | `"us-east-1"` | Provider, SSM, ECR. |
| `envs` | `["dev", "prod"]` | Envs to generate. |
| `domain_name_dev` / `domain_name_prod` | `"dev-app.example.com"` / `"app.example.com"` | ACM, Route53. |
| `hosted_zone_id` | `"Z1234567890ABCDE"` | Route53 validation and alias. |
| `alarm_email_dev` / `alarm_email_prod` | Email addresses | SNS subscriptions. |
| `vpc_cidr_dev` / `vpc_cidr_prod` | `"10.20.0.0/16"` / `"10.30.0.0/16"` | VPC and subnets. |
| `public_subnets` / `private_subnets` | Lists of CIDRs | Subnets per env. |
| `instance_type`, `min_size`, `max_size`, `desired_capacity` | e.g. `"t3.micro"`, 1, 2, 1 (dev) and 2, 4, 2 (prod) | Launch template, ASG. |
| `ami_id` | `""` or AMI ID | Optional override. |

You can extend this (e.g. log retention, alarm thresholds); the crew uses these values to fill variables and tfvars.

### 10.2 Agents

Define agents that **write** files and **confirm**:

| Agent | Role | Goal |
|-------|------|------|
| **InfraAuthor** | Writes Terraform | Generate bootstrap (S3, DynamoDB, KMS, CloudTrail bucket), platform module (VPC, ALB, HTTPS, ACM, Route53, ECR, SSM, CloudWatch Agent, ASG blue/green, CodeDeploy, alarms, security services), and dev/prod envs (backend.hcl, main.tf, tfvars) from the requirements. |
| **AppAuthor** | Writes app + Docker | Generate `app/package.json`, `app/server.js`, `app/Dockerfile` (Node/Express, /health and /, Dockerfile as in Step 2). |
| **DeployAuthor** | Writes CodeDeploy bundle | Generate `deploy/appspec.yml` and `deploy/scripts/` (install.sh, stop.sh, start.sh, validate.sh) with SSM paths that include env (e.g. `/bluegreen/prod/ecr_repo_name`). |
| **CIAuthor** | Writes GitHub Actions | Generate `.github/workflows/` (terraform-plan.yml, terraform-apply.yml, build-push.yml, deploy.yml) with OIDC and env-specific SSM paths. |
| **VerifierAgent** | Validates and confirms | Run `terraform validate` in bootstrap and each env; optionally `docker build` in app/; produce a run-order summary and, if requested, post-apply checks (HTTPS health, SSM). |

Each agent needs **tools** that can **write files** (e.g. a tool that takes path + content and writes to the repo). Optionally give InfraAuthor or VerifierAgent **Code Interpreter** so they can run `terraform validate` and `docker build` in the generated repo.

### 10.3 Task sequence (orchestrate then confirm)

Run tasks in this order so outputs of earlier tasks are available to later ones:

1. **Parse requirements and create repo structure** — Create directories: `app/`, `deploy/scripts/`, `infra/bootstrap/`, `infra/modules/platform/`, `infra/envs/dev/`, `infra/envs/prod/`, `.github/workflows/`, `crewai/`.
2. **Write bootstrap Terraform** — InfraAuthor writes `infra/bootstrap/variables.tf`, `main.tf`, `outputs.tf` (S3 state bucket, DynamoDB lock, KMS, CloudTrail bucket + policy). Use project and region from requirements.
3. **Write platform module** — InfraAuthor writes `infra/modules/platform/variables.tf`, `main.tf`, `outputs.tf` (full platform as in Steps 4–5: VPC, ALB, HTTPS listener + HTTP redirect, ACM + Route53, ECR, SSM, CloudWatch Agent config and log groups, user data, ASG blue/green, CodeDeploy, alarms, security services). Use requirement values for domain, hosted zone, alarm email, VPC CIDRs, instance type, min/max/desired, cloudtrail_bucket (variable).
4. **Write dev and prod envs** — InfraAuthor writes for each env: `backend.hcl` (with placeholders `REPLACE_WITH_BOOTSTRAP_TFSTATE_BUCKET`, `REPLACE_WITH_BOOTSTRAP_TFLOCK_TABLE`), `main.tf` (backend "s3", provider, module "platform" with env and vars), `dev.tfvars` / `prod.tfvars` (cloudtrail_bucket placeholder, domain, VPC, instance settings from requirements).
5. **Write app and Dockerfile** — AppAuthor writes `app/package.json`, `app/server.js`, `app/Dockerfile` (content as in Step 2).
6. **Write deploy bundle** — DeployAuthor writes `deploy/appspec.yml` and the four scripts (install, stop, start, validate) with env-aware SSM paths (e.g. `/bluegreen/prod/ecr_repo_name`, `/bluegreen/prod/image_tag`).
7. **Write GitHub Actions** — CIAuthor writes the four workflow files (plan on PR for infra, apply on push to main for prod, build-push to ECR + SSM tag, deploy on workflow_run) with OIDC and region/repo from requirements.
8. **Write CrewAI verifier (optional)** — VerifierAgent or a small task writes `crewai/requirements.txt`, `tools.py` (HTTPS health check, optional SSM read), `agents.py`, `flow.py`, `run.py` so the user can run the crew later to confirm HTTPS and SSM.
9. **Confirm** — VerifierAgent runs `terraform validate` in `infra/bootstrap`, `infra/envs/dev`, `infra/envs/prod`; optionally `docker build -t test ./app`; outputs a **report**: list of generated files, validation results, and **run order** (bootstrap apply → copy outputs to backend.hcl and tfvars → dev apply → prod apply → push to main → validate HTTPS). Optionally a second phase **post-apply confirm**: user runs apply and CI, then crew is invoked again with prod URL and region to check `https://<domain>/health` and SSM parameters.

### 10.4 Tools the crew needs

- **File write tool** — Input: file path (relative to repo root), content (string). Writes the file. Agents use this to create every file under app/, deploy/, infra/, .github/, crewai/.
- **File read tool** (optional) — So an agent can read bootstrap outputs or existing files before generating env config.
- **Code Interpreter** (optional) — So VerifierAgent can run `terraform validate` and `docker build` inside the generated repo. If you don’t use Code Interpreter, the “confirm” step can be a checklist the crew outputs (user runs validate and build manually).
- **HTTPS / SSM tools** (for post-apply confirm) — Same as Step 8: request to `/health`, read SSM params. Used only in the optional second run after the user has applied.

### 10.5 Safety

- **Generation only by default** — The crew **writes** Terraform and config; it does **not** run `terraform apply` or push to Git unless you explicitly add a task and approval (e.g. human-in-the-loop or sandbox-only apply).
- **Placeholders** — Bootstrap outputs (bucket name, lock table, CloudTrail bucket) are not known until after the first apply; use placeholders in backend.hcl and cloudtrail_bucket in tfvars, and tell the user to fill them after bootstrap (as in Step 9).
- **Confirm = validate + optional verify** — “Confirm” here means: validate generated files and output run order; optionally, after user applies, verify HTTPS and SSM.

### 10.6 Summary

| Step | Who | What |
|------|-----|------|
| User | Provides | Requirements (project, region, domains, zones, emails, VPC, instance, ASG sizes). |
| Crew | Writes | Terraform (bootstrap, platform, dev/prod), app (package.json, server.js, Dockerfile), deploy (appspec, scripts), .github/workflows, optional crewai/. |
| Crew | Confirms | Terraform validate, optional docker build; run-order summary; optional post-apply HTTPS + SSM check. |
| User | Runs | Bootstrap apply → fill backend.hcl and tfvars → dev apply → prod apply → push to main → validate. |

For the exact file contents the crew should generate, follow the rest of this guide (Steps 1–8); the crew’s job is to produce those files from the requirements and then confirm.

---

## Checklist Summary

**Manual build (Steps 1–9):**

- [ ] Create repo and folder structure (app, deploy/scripts, infra/bootstrap, infra/modules/platform, infra/envs/dev and prod, .github/workflows, crewai).
- [ ] Create app: package.json, server.js, Dockerfile.
- [ ] Create deploy: appspec.yml, install.sh, stop.sh, start.sh, validate.sh (SSM paths match env: e.g. /bluegreen/prod/ecr_repo_name and image_tag).
- [ ] Create infra bootstrap: variables.tf, main.tf (S3 state, DynamoDB lock, KMS, CloudTrail bucket + policy), outputs.tf; run apply; fill backend.hcl and cloudtrail_bucket in envs.
- [ ] Create platform module: variables.tf (include cloudtrail_bucket), main.tf (VPC, ALB, HTTPS, Route53, ECR, SSM, CW agent, ASG blue/green, CodeDeploy, alarms, security services), outputs.tf.
- [ ] Create envs: dev and prod backend.hcl, main.tf (call platform module), dev.tfvars, prod.tfvars; run init + apply for both.
- [ ] Create GitHub Actions: terraform-plan.yml, terraform-apply.yml, build-push.yml, deploy.yml; set AWS_ROLE_TO_ASSUME and AWS_REGION.
- [ ] Create crewai: requirements.txt, tools.py (e.g. HTTPS check, SSM read), agents.py, flow.py, run.py.
- [ ] Run order: bootstrap → dev apply → prod apply → push to main → validate HTTPS and logs.

**Orchestrator (Step 10):** User provides requirements → crew writes Terraform, Dockerfile, app, deploy, workflows (and optional crewai verifier) → crew confirms (terraform validate, optional docker build; run-order summary; optional post-apply HTTPS + SSM check).

**Optional:** Release & Deployment Pipeline (release-crew/, release-prep.yml, CHANGELOG, deploy_checklist, rollback_plan, test_plan) — **AWS_EC2_BLUEGREEN_EXPLAINED.md** §18. **Production hardening** (what’s included and what to add): **AWS_EC2_BLUEGREEN_EXPLAINED.md** §19.

For a **detailed explanation** of each component (Blue/Green, ALB, ACM, CloudWatch, security services, backend), see **EXPLAIN_AWS_EC2_BLUEGREEN_BEGINNER.md**. For **full code samples**, **split Terraform structure**, **production hardening**, **Release Pipeline**, and **extended CrewAI orchestration**, see **AWS_EC2_BLUEGREEN_EXPLAINED.md**.
